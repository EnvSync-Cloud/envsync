# Production Docker Compose for Dokploy / self-hosting
# Usage: docker compose -f docker-compose.prod.yaml up -d
# Requires: .env file with production values (see .env.prod.example)

services:
  envsync_api:
    image: ghcr.io/envsync-cloud/envsync-api:latest
    env_file:
      - .env
    ports:
      - "${PORT:-4000}:${PORT:-4000}"
    restart: unless-stopped
    environment:
      NODE_ENV: production
      REDIS_URL: redis://redis:6379
      S3_ENDPOINT: http://rustfs:9001
      S3_BUCKET_URL: http://rustfs:9001
      KEYCLOAK_URL: http://keycloak:8080
      OTEL_EXPORTER_OTLP_ENDPOINT: http://otel-collector:4318
    depends_on:
      redis:
        condition: service_healthy
      rustfs:
        condition: service_started
      keycloak:
        condition: service_healthy
    networks:
      - envsync_network

  envsync_init:
    image: ghcr.io/envsync-cloud/envsync-api:latest
    command: ["bun", "run", "scripts/prod-init.ts"]
    env_file:
      - .env
    environment:
      S3_ENDPOINT: http://rustfs:9001
      KEYCLOAK_URL: http://keycloak:8080
    depends_on:
      redis:
        condition: service_healthy
      keycloak:
        condition: service_healthy
      rustfs:
        condition: service_started
    restart: "no"
    networks:
      - envsync_network

  redis:
    image: redis:latest
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    volumes:
      - redis_data:/data
    networks:
      - envsync_network

  rustfs:
    image: rustfs/rustfs:latest
    restart: unless-stopped
    environment:
      RUSTFS_DATA_DIR: /data
      RUSTFS_ACCESS_KEY: ${RUSTFS_ACCESS_KEY:-rustfsadmin}
      RUSTFS_SECRET_KEY: ${RUSTFS_SECRET_KEY:-rustfsadmin}
      RUSTFS_CONSOLE_ENABLE: "false"
    volumes:
      - rustfs_data:/data
    networks:
      - envsync_network

  # Keycloak DB: PostgreSQL for Keycloak's own storage only
  keycloak_db:
    image: postgres:17
    restart: unless-stopped
    environment:
      POSTGRES_USER: keycloak
      POSTGRES_PASSWORD: ${KEYCLOAK_DB_PASSWORD:-keycloak}
      POSTGRES_DB: keycloak
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U keycloak"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    volumes:
      - keycloak_db_data:/var/lib/postgresql/data
    networks:
      - envsync_network

  keycloak:
    image: quay.io/keycloak/keycloak:26.1
    restart: unless-stopped
    command: ["start", "--import-realm", "--optimized"]
    environment:
      KC_DB: postgres
      KC_DB_URL: jdbc:postgresql://keycloak_db:5432/keycloak
      KC_DB_USERNAME: keycloak
      KC_DB_PASSWORD: ${KEYCLOAK_DB_PASSWORD:-keycloak}
      KC_HOSTNAME: ${KEYCLOAK_HOSTNAME}
      KC_HTTP_ENABLED: ${KEYCLOAK_HTTP_ENABLED:-false}
      KC_PROXY_HEADERS: ${KEYCLOAK_PROXY_HEADERS:-xforwarded}
      KC_FEATURES: device-flow
      KEYCLOAK_ADMIN: ${KEYCLOAK_ADMIN_USER:-admin}
      KEYCLOAK_ADMIN_PASSWORD: ${KEYCLOAK_ADMIN_PASSWORD}
    ports:
      - "${KEYCLOAK_PORT:-8080}:8080"
    healthcheck:
      test: ["CMD-SHELL", "exec 3<>/dev/tcp/localhost/8080 && echo -e 'GET /health/ready HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: close\\r\\n\\r\\n' >&3 && cat <&3 | grep -q '200'"]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 60s
    volumes:
      - ./docker/keycloak/realm-export.json:/opt/keycloak/data/import/realm-export.json:ro
    depends_on:
      keycloak_db:
        condition: service_healthy
    networks:
      - envsync_network

  spacetimedb:
    image: clockworklabs/spacetimedb:latest
    profiles: ["stdb"]
    restart: unless-stopped
    volumes:
      - stdb_data:/stdb
    environment:
      SPACETIMEDB_LOG_LEVEL: info
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/v1/ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - envsync_network

  stdb_publish:
    build:
      context: packages/envsync-stdb
      dockerfile: Dockerfile
    profiles: ["stdb"]
    depends_on:
      spacetimedb:
        condition: service_healthy
    networks:
      - envsync_network
    restart: "no"

  # --- Observability Stack (Grafana LGTM) ---
  tempo:
    image: grafana/tempo:latest
    restart: unless-stopped
    command: ["-config.file=/etc/tempo.yaml"]
    volumes:
      - ./docker/otel/tempo-config.yaml:/etc/tempo.yaml:ro
      - tempo_data:/var/tempo
    networks:
      - envsync_network

  loki:
    image: grafana/loki:3
    restart: unless-stopped
    command: ["-config.file=/etc/loki/local-config.yaml"]
    volumes:
      - ./docker/otel/loki-config.yaml:/etc/loki/local-config.yaml:ro
      - loki_data:/loki
    networks:
      - envsync_network

  prometheus:
    image: prom/prometheus:latest
    restart: unless-stopped
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--web.enable-remote-write-receiver"
    volumes:
      - ./docker/otel/prometheus.yaml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - envsync_network

  otel-collector:
    image: otel/opentelemetry-collector-contrib:latest
    restart: unless-stopped
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./docker/otel/otel-collector-config.yaml:/etc/otel-collector-config.yaml:ro
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "2"
        reservations:
          memory: 256M
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:13133"]
      interval: 10s
      timeout: 5s
      retries: 5
    depends_on:
      tempo:
        condition: service_started
      loki:
        condition: service_started
      prometheus:
        condition: service_healthy
    networks:
      - envsync_network

  promtail:
    image: grafana/promtail:3
    restart: unless-stopped
    command: ["-config.file=/etc/promtail/config.yaml"]
    volumes:
      - ./docker/otel/promtail-config.yaml:/etc/promtail/config.yaml:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    depends_on:
      loki:
        condition: service_started
    networks:
      - envsync_network

  grafana:
    image: grafana/grafana:latest
    restart: unless-stopped
    ports:
      - "${GRAFANA_PORT:-3302}:3000"
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD:-EnvSync@2024!}
      GF_AUTH_ANONYMOUS_ENABLED: "true"
      GF_AUTH_ANONYMOUS_ORG_ROLE: Viewer
    volumes:
      - grafana_data:/var/lib/grafana
      - ./docker/otel/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./docker/otel/dashboards:/var/lib/grafana/dashboards:ro
    depends_on:
      prometheus:
        condition: service_healthy
      tempo:
        condition: service_started
      loki:
        condition: service_started
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - envsync_network

  # --- HyperDX (Session Replay + Observability) ---
  hdx:
    image: clickhouse/clickstack-all-in-one:latest
    restart: unless-stopped
    volumes:
      - hdx_data:/data/db
      - hdx_ch_data:/var/lib/clickhouse
      - hdx_ch_logs:/var/log/clickhouse-server
    environment:
      - OTEL_AGENT_FEATURE_GATE_ARG='--feature-gates=clickhouse.json'
      - BETA_CH_OTEL_JSON_SCHEMA_ENABLED=true
    ports:
      - "${HDX_PORT:-8800}:8080"
      - "${HDX_OTEL_GRPC_PORT:-4316}:4317"
      - "${HDX_OTEL_PORT:-4319}:4318"
    networks:
      - envsync_network

networks:
  envsync_network:
    driver: bridge

volumes:
  redis_data:
  rustfs_data:
  stdb_data:
  keycloak_db_data:
  tempo_data:
  loki_data:
  prometheus_data:
  grafana_data:
  hdx_data:
  hdx_ch_data:
  hdx_ch_logs:
