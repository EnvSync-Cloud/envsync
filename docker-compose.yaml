services:
  # envsync_api:
  #   build:
  #     context: packages/envsync-api
  #     dockerfile: Dockerfile
  #   env_file:
  #     - .env
  #   ports:
  #     - "${PORT:-4000}:${PORT:-4000}"
  #   restart: always
  #   environment:
  #     ZITADEL_URL: http://zitadel:8080
  #   depends_on:
  #     - postgres
  #     - redis
  #     - rustfs
  #     - mailpit
  #     - zitadel
  #     - vault
  #   networks:
  #     - envsync_network

  minikms_db:
    image: postgres:17
    restart: unless-stopped
    ports:
      - "${MINIKMS_DB_PORT:-5544}:5432"
    volumes:
      - minikms_db_data:/var/lib/postgresql/data
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
      POSTGRES_DB: minikms
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres} -d minikms"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - envsync_network

  minikms_migrate:
    image: postgres:17
    command: ["psql", "-h", "minikms_db", "-U", "postgres", "-d", "minikms", "-f", "/migrations/001_initial_schema.sql"]
    environment:
      PGPASSWORD: ${POSTGRES_PASSWORD:-postgres}
    volumes:
      - ./docker/minikms/migrations:/migrations:ro
    depends_on:
      minikms_db:
        condition: service_healthy
    networks:
      - envsync_network
    restart: "no"

  minikms:
    image: ghcr.io/envsync-cloud/minikms:latest
    ports:
      - "${MINIKMS_GRPC_PORT:-50051}:50051"
    restart: always
    environment:
      MINIKMS_DB_URL: "postgres://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@minikms_db:5432/minikms?sslmode=disable"
      MINIKMS_REDIS_URL: "redis://redis:6379/1"
      MINIKMS_ROOT_KEY: ${MINIKMS_ROOT_KEY}
      MINIKMS_GRPC_ADDR: "0.0.0.0:50051"
    depends_on:
      minikms_migrate:
        condition: service_completed_successfully
      redis:
        condition: service_started
    networks:
      - envsync_network

  postgres:
    image: postgres:17
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    restart: always
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
      POSTGRES_DB: ${POSTGRES_DB:-envsync}
    networks:
      - envsync_network

  rustfs:
    image: rustfs/rustfs:latest
    ports:
      - "${RUSTFS_PORT:-19000}:9000"
      - "${RUSTFS_HTTP_PORT:-19001}:9001"
    restart: always
    volumes:
      - rustfs_data:/data
    environment:
      RUSTFS_DATA_DIR: /data
      RUSTFS_ACCESS_KEY: ${RUSTFS_ACCESS_KEY:-rustfsadmin}
      RUSTFS_SECRET_KEY: ${RUSTFS_SECRET_KEY:-rustfsadmin}
      RUSTFS_CONSOLE_ENABLE: "true"
    networks:
      - envsync_network

  mailpit:
    image: axllent/mailpit:latest
    container_name: mailpit
    restart: unless-stopped
    volumes:
      - mailpit_data:/data
    ports:
      - "${MAILPIT_PORT:-8025}:8025"
      - "${MAILPIT_SMTP_PORT:-1025}:1025"
    environment:
      MP_MAX_MESSAGES: 5000
      MP_DATABASE: /data/mailpit.db
      MP_SMTP_AUTH_ACCEPT_ANY: 1
      MP_SMTP_AUTH_ALLOW_INSECURE: 1
    networks:
      - envsync_network

  # Zitadel DB: init uses default DB "postgres"; Zitadel creates "zitadel" on first start.
  # See https://github.com/zitadel/zitadel/blob/main/apps/docs/content/self-hosting/deploy/docker-compose.yaml
  zitadel_db:
    image: postgres:17
    restart: unless-stopped
    environment:
      PGUSER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres}"]
      interval: 10s
      timeout: 30s
      retries: 5
      start_period: 20s
    volumes:
      - zitadel_db_data:/var/lib/postgresql/data
    networks:
      - envsync_network

  zitadel:
    image: ghcr.io/zitadel/zitadel:latest
    restart: unless-stopped
    command: start-from-init --masterkey "${ZITADEL_MASTERKEY:-MasterkeyNeedsToHave32Characters}"
    user: "0"
    environment:
      ZITADEL_EXTERNALDOMAIN: ${ZITADEL_EXTERNALDOMAIN:-localhost}
      ZITADEL_EXTERNALSECURE: "false"
      ZITADEL_TLS_ENABLED: "false"
      ZITADEL_DATABASE_POSTGRES_HOST: zitadel_db
      ZITADEL_DATABASE_POSTGRES_PORT: "5432"
      ZITADEL_DATABASE_POSTGRES_DATABASE: zitadel
      ZITADEL_DATABASE_POSTGRES_ADMIN_USERNAME: ${POSTGRES_USER:-postgres}
      ZITADEL_DATABASE_POSTGRES_ADMIN_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
      ZITADEL_DATABASE_POSTGRES_ADMIN_SSL_MODE: disable
      ZITADEL_DATABASE_POSTGRES_USER_USERNAME: zitadel
      ZITADEL_DATABASE_POSTGRES_USER_PASSWORD: zitadel
      ZITADEL_DATABASE_POSTGRES_USER_SSL_MODE: disable
      ZITADEL_FIRSTINSTANCE_ORG_HUMAN_PASSWORDCHANGEREQUIRED: "false"
      ZITADEL_FIRSTINSTANCE_ORG_HUMAN_USERNAME: ${ZITADEL_ADMIN_USERNAME:-zitadel-admin}
      ZITADEL_FIRSTINSTANCE_ORG_HUMAN_PASSWORD: ${ZITADEL_ADMIN_PASSWORD:-Password1!}
      ZITADEL_FIRSTINSTANCE_ORG_NAME: EnvSync
      ZITADEL_FIRSTINSTANCE_PATPATH: /current-dir/admin.pat
      ZITADEL_FIRSTINSTANCE_ORG_MACHINE_MACHINE_USERNAME: admin
      ZITADEL_FIRSTINSTANCE_ORG_MACHINE_MACHINE_NAME: EnvSync IAM Owner
      ZITADEL_FIRSTINSTANCE_ORG_MACHINE_PAT_EXPIRATIONDATE: "2029-01-01T00:00:00Z"
      ZITADEL_FIRSTINSTANCE_LOGINCLIENTPATPATH: /current-dir/login-client.pat
      ZITADEL_FIRSTINSTANCE_ORG_LOGINCLIENT_MACHINE_USERNAME: login-client
      ZITADEL_FIRSTINSTANCE_ORG_LOGINCLIENT_MACHINE_NAME: Automatically Initialized IAM_LOGIN_CLIENT
      ZITADEL_FIRSTINSTANCE_ORG_LOGINCLIENT_PAT_EXPIRATIONDATE: "2029-01-01T00:00:00Z"
      ZITADEL_DEFAULTINSTANCE_FEATURES_LOGINV2_REQUIRED: "true"
      ZITADEL_DEFAULTINSTANCE_FEATURES_LOGINV2_BASEURI: http://localhost:3000/ui/v2/login/
      ZITADEL_OIDC_DEFAULTLOGINURLV2: http://localhost:3000/ui/v2/login/login?authRequest=
      ZITADEL_OIDC_DEFAULTLOGOUTURLV2: http://localhost:3000/ui/v2/login/logout?post_logout_redirect=
      ZITADEL_SAML_DEFAULTLOGINURLV2: http://localhost:3000/ui/v2/login/login?samlRequest=
    volumes:
      # Named volume: PATs persist but are not on host. For host-readable PATs use: ./zitadel-data:/current-dir:delegated
      - zitadel_data:/current-dir
    ports:
      - "${ZITADEL_PORT:-8080}:8080"
      - "${ZITADEL_LOGIN_PORT:-3000}:3000"
    healthcheck:
      test: ["CMD", "/app/zitadel", "ready"]
      interval: 10s
      timeout: 60s
      retries: 5
      start_period: 10s
    depends_on:
      zitadel_db:
        condition: service_healthy
    networks:
      - envsync_network

  zitadel_login:
    image: ghcr.io/zitadel/zitadel-login:latest
    restart: unless-stopped
    user: "0"
    environment:
      # With network_mode: service:zitadel, use localhost (same container network namespace)
      ZITADEL_API_URL: http://localhost:8080
      NEXT_PUBLIC_BASE_PATH: /ui/v2/login
      ZITADEL_SERVICE_USER_TOKEN_FILE: /current-dir/login-client.pat
    network_mode: service:zitadel
    volumes:
      - zitadel_data:/current-dir:ro
    depends_on:
      zitadel:
        condition: service_healthy
        restart: false

  vault-init:
    image: alpine:latest
    command: sh -c "chown -R 100:1000 /vault/data"
    volumes:
      - vault_data:/vault/data
    restart: "no"

  vault:
    image: hashicorp/vault:latest
    command: vault server -config=/vault/config/vault.hcl
    ports:
      - "${VAULT_PORT:-8200}:8200"
    restart: always
    environment:
      VAULT_ADDR: http://localhost:8200
      VAULT_TOKEN: ${VAULT_TOKEN:-}
      SKIP_SETCAP: "true"
      SKIP_CHOWN: "true"
    volumes:
      - vault_data:/vault/data
      - ./docker/vault/vault.hcl:/vault/config/vault.hcl:ro
    depends_on:
      vault-init:
        condition: service_completed_successfully
      postgres:
        condition: service_started
      redis:
        condition: service_started
      zitadel:
        condition: service_started
    networks:
      - envsync_network

  openfga_db:
    image: postgres:17
    restart: unless-stopped
    environment:
      POSTGRES_USER: openfga
      POSTGRES_PASSWORD: openfga
      POSTGRES_DB: openfga
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U openfga"]
      interval: 5s
      timeout: 5s
      retries: 5
    volumes:
      - openfga_db_data:/var/lib/postgresql/data
    networks:
      - envsync_network

  openfga_migrate:
    image: openfga/openfga:latest
    command: migrate
    environment:
      OPENFGA_DATASTORE_ENGINE: postgres
      OPENFGA_DATASTORE_URI: "postgres://openfga:openfga@openfga_db:5432/openfga?sslmode=disable"
    depends_on:
      openfga_db:
        condition: service_healthy
    networks:
      - envsync_network
    restart: "no"

  openfga:
    image: openfga/openfga:latest
    command: run
    ports:
      - "${OPENFGA_HTTP_PORT:-8090}:8090"
      - "${OPENFGA_GRPC_PORT:-8091}:8091"
      - "${OPENFGA_PLAYGROUND_PORT:-8092}:8092"
    environment:
      OPENFGA_DATASTORE_ENGINE: postgres
      OPENFGA_DATASTORE_URI: "postgres://openfga:openfga@openfga_db:5432/openfga?sslmode=disable"
      OPENFGA_PLAYGROUND_ENABLED: "true"
      OPENFGA_PLAYGROUND_PORT: ${OPENFGA_PLAYGROUND_PORT:-8092}
      OPENFGA_HTTP_ADDR: ${OPENFGA_HTTP_ADDR:-0.0.0.0:8090}
      OPENFGA_GRPC_ADDR: ${OPENFGA_GRPC_ADDR:-0.0.0.0:8091}
    depends_on:
      openfga_migrate:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "/usr/local/bin/grpc_health_probe", "-addr=:8091"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "1"
    networks:
      - envsync_network
    restart: always

  # --- Observability Stack (Grafana LGTM) ---
  tempo:
    image: grafana/tempo:2.6.1
    command: ["-config.file=/etc/tempo.yaml"]
    volumes:
      - ./docker/otel/tempo-config.yaml:/etc/tempo.yaml:ro
      - tempo_data:/var/tempo
    ports:
      - "${TEMPO_PORT:-3200}:3200"
    networks:
      - envsync_network

  loki:
    image: grafana/loki:3
    command: ["-config.file=/etc/loki/local-config.yaml"]
    volumes:
      - ./docker/otel/loki-config.yaml:/etc/loki/local-config.yaml:ro
      - loki_data:/loki
    networks:
      - envsync_network

  prometheus:
    image: prom/prometheus:latest
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--web.enable-remote-write-receiver"
    volumes:
      - ./docker/otel/prometheus.yaml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - envsync_network

  otel-collector:
    image: otel/opentelemetry-collector-contrib:latest
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./docker/otel/otel-collector-config.yaml:/etc/otel-collector-config.yaml:ro
    ports:
      - "${OTEL_COLLECTOR_GRPC_PORT:-4317}:4317"
      - "${OTEL_COLLECTOR_HTTP_PORT:-4318}:4318"
      - "8888:8888"
      - "13133:13133"
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "2"
        reservations:
          memory: 256M
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:13133"]
      interval: 10s
      timeout: 5s
      retries: 5
    depends_on:
      tempo:
        condition: service_started
      loki:
        condition: service_started
      prometheus:
        condition: service_healthy
    networks:
      - envsync_network

  promtail:
    image: grafana/promtail:3
    command: ["-config.file=/etc/promtail/config.yaml"]
    volumes:
      - ./docker/otel/promtail-config.yaml:/etc/promtail/config.yaml:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    depends_on:
      loki:
        condition: service_started
    networks:
      - envsync_network

  grafana:
    image: grafana/grafana:latest
    ports:
      - "${GRAFANA_PORT:-3302}:3000"
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD:-EnvSync@2024!}
      GF_AUTH_ANONYMOUS_ENABLED: "true"
      GF_AUTH_ANONYMOUS_ORG_ROLE: Viewer
    volumes:
      - grafana_data:/var/lib/grafana
      - ./docker/otel/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./docker/otel/dashboards:/var/lib/grafana/dashboards:ro
    depends_on:
      prometheus:
        condition: service_healthy
      tempo:
        condition: service_started
      loki:
        condition: service_started
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - envsync_network

  redis:
    image: ghcr.io/envsync-cloud/chabi:0.1.0-rc3
    restart: always
    ports:
      - "${REDIS_PORT:-6379}:6379"
    networks:
      - envsync_network
    volumes:
      - redis_data:/data

  httpbin:
    image: kong/httpbin:latest
    ports:
      - "${HTTPBIN_PORT:-8181}:80"
    networks:
      - envsync_network

  # --- HyperDX (Session Replay + Observability) ---
  hdx:
    image: clickhouse/clickstack-all-in-one:latest
    volumes:
      - hdx_data:/data/db
      - hdx_ch_data:/var/lib/clickhouse
      - hdx_ch_logs:/var/log/clickhouse-server
    restart: unless-stopped
    environment:
      - OTEL_AGENT_FEATURE_GATE_ARG='--feature-gates=clickhouse.json'
      - BETA_CH_OTEL_JSON_SCHEMA_ENABLED=true
    ports:
      - "${HDX_PORT:-8800}:8080"
      - "${HDX_OTEL_GRPC_PORT:-4316}:4317"
      - "${HDX_OTEL_PORT:-4319}:4318"
    networks:
      - envsync_network

networks:
  envsync_network:
    driver: bridge

volumes:
  postgres_data:
  redis_data:
  mailpit_data:
  vault_data:
  rustfs_data:
  zitadel_db_data:
  zitadel_data:
  openfga_db_data:
  tempo_data:
  loki_data:
  prometheus_data:
  grafana_data:
  hdx_data:
  hdx_ch_data:
  hdx_ch_logs:
  minikms_db_data:
